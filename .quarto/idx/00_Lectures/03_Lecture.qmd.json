{"title":"Lecture 3 - Deployment and Production Considerations","markdown":{"yaml":{"title":"Lecture 3 - Deployment and Production Considerations","subtitle":"Deploying Machine Learning Applications","author":"Ményssa Cherifa-Luron","date":"today"},"headingText":"TL;DR:","containsRefs":false,"markdown":"\n\n\nIf you're short on time, tune into the cosmic highlights of our course in this audio summary:\n\n<div style=\"text-align: center;\">\n  <figure>  \n    <figcaption>Listen to the Audio Overview:</figcaption>  \n    <audio controls src=\"03_Podcast.wav\"></audio>  \n    <br>\n    <button class=\"download-btn\" onclick=\"window.location.href='03_Podcast.wav'\">Download audio</button>  \n  </figure>\n</div>\n\nIn this course, we embark on a stellar journey to master the deployment of machine learning applications:\n\n- **Containerization** with Docker: Your starship for consistent and portable deployments.\n- **CI/CD Pipelines** using GitHub Actions: Automate your deployment journey like a seasoned Jedi.\n- **Heroku Deployment**: Launch your applications into the cloud galaxy with ease.\n- **Best Practices**: Optimize, scale, and secure your deployments for interstellar performance.\n- **Advanced Tools**: Explore the vast universe of cloud platforms and orchestration tools.\n\n---\n\n**Your deployment is a starship.**\n\nEach component is a crucial part of your mission, from the engines (Docker) to the navigation system (GitHub Actions).\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.science-et-vie.com/wp-content/uploads/scienceetvie/2024/03/starship_vol_3_une.jpg\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Containerization** is like building a reliable starship that can travel across different galaxies (environments) without a hitch. **CI/CD Pipelines** are your autopilot, ensuring smooth and automated journeys through the deployment cosmos.\n\n**Heroku** is your launchpad, propelling your applications into the cloud with the grace of a Jedi starfighter. **Best Practices** are your navigational charts, guiding you through optimization, scaling, and security challenges.\n\n**Advanced Tools** are the hyperdrive enhancements, allowing you to explore new frontiers in cloud deployment and orchestration.\n\n> Are you ready to launch your machine learning applications into the cloud galaxy?\n\n**Let's get started on this epic adventure!**\n\n## 1. Introduction to Deploying Machine Learning Applications\n\n### 1.1. The Challenges of Deploying Machine Learning Models\n\nDeploying machine learning models can sometimes feel like trying to fit a square peg into a round hole. Unlike traditional software, these models are like your favorite rock band—dynamic, unpredictable, and requiring lots of fine-tuning for each performance.\n\nLet's explore these challenges in detail:\n\n**Environment Mismatch:**\nImagine your model is a Broadway star, rehearsed and fine-tuned in one environment, but on opening night, the stage looks completely different.\n\nThis mismatch is one of the most common issues faced during deployment. \n\nDifferent machines might have various operating systems, library versions, or hardware configurations, causing unexpected behavior when the model is deployed. This is akin to a musician arriving at a venue only to find their instruments are tuned differently or missing entirely.\n\nTo mitigate this, it's essential to maintain a consistent environment across development and production stages. This can be achieved through environment management tools like virtualenv for Python or using containerization technologies like Docker, which we will explore later.\n\n**Scalability Issues:**\nPicture a concert where more fans show up than the venue can handle. Your model should be ready to handle the crowd, scaling up or down as needed. Scalability is crucial, especially in cloud-based applications where the user base can grow unpredictably. \n\nWithout proper scalability, applications can suffer from slow response times or crashes under heavy load.\n\nTo address scalability, load testing tools like Apache JMeter or Locust can be used to simulate a range of traffic conditions. Additionally, cloud platforms like AWS, Azure, and Google Cloud offer auto-scaling features that automatically adjust resources based on demand.\n\n**Version Control:**\nLike a band releasing multiple albums, tracking different versions of your models is crucial to know which one hits the right note. Imagine if every time a band played, they had to remember which version of their song arrangement they were supposed to perform. Similarly, in machine learning, keeping track of model versions ensures that you can revert to previous versions if a new model does not perform as expected.\n\nTools like DVC (Data Version Control) and MLflow provide mechanisms to manage and version control models, datasets, and experiments, making it easier to reproduce results and manage model lifecycle.\n\n**Performance Optimization:**\nEnsuring your model performs efficiently is like ensuring the lead singer hits all the right notes during a live performance. Performance can be influenced by factors such as model complexity, data input size, and computational resources. Optimizing performance is crucial for providing a seamless user experience and can also reduce computational costs.\n\nTechniques such as quantization, pruning, and knowledge distillation can be applied to reduce model size and improve inference speed without significantly sacrificing accuracy. Profiling tools like TensorBoard and PyTorch's profiler can help identify performance bottlenecks in your model.\n\n### 1.2. Different Deployment Approaches\n\n**Deploying models isn't a one-size-fits-all approach.** \n\nIt's more like choosing between a solo acoustic set or a full-on rock concert. Each deployment strategy has its pros and cons, depending on the specific requirements and constraints of the application.\n\nLet’s explore these different paths in detail:\n\n**Local Deployment:**\nIt's like playing music for your friends in your garage. Quick and easy, but not scalable. Local deployment is suitable for testing and development purposes, allowing you to quickly iterate and debug models on your local machine. However, it lacks the scalability and reliability needed for production environments.\n\nFor local deployment, tools like Jupyter Notebooks or local Flask servers can be used to serve models and test their endpoints. This approach is ideal for prototyping and learning but not for handling production-scale traffic.\n\n**Server-Based Deployment:**\nImagine booking a local venue. More organized and can handle a decent crowd, but may require manual tuning when issues arise. Server-based deployment involves hosting your model on dedicated servers, either on-premises or in the cloud. This approach provides more control over the environment and resources.\n\nServers can be configured using web frameworks like Flask or Django in combination with WSGI servers like Gunicorn. This setup provides flexibility and control, but requires manual management of scalability, load balancing, and failover mechanisms.\n\n**Cloud Deployment:**\nNow, we're talking major festival level. Your model gets the flexibility and power of cloud resources, scaling up and down with ease. Cloud deployment is the go-to solution for applications requiring high availability and scalability. Cloud platforms like AWS SageMaker, Azure Machine Learning, and Google AI Platform offer managed services for deploying and scaling models effortlessly.\n\nWith cloud deployment, you can leverage features like auto-scaling, load balancing, and continuous integration/continuous deployment (CI/CD) pipelines to ensure your model is always available and up-to-date. Additionally, cloud providers offer a variety of instance types, allowing you to choose the best resources for your model's needs.\n\n**Edge Deployment:**\nThink of it as a secret pop-up concert. Models are deployed closer to where the action happens, reducing latency and bandwidth use. Edge deployment is ideal for applications requiring real-time inference or operating in environments with limited connectivity.\n\nEdge devices, such as IoT devices, smartphones, or embedded systems, can run machine learning models using frameworks like TensorFlow Lite or ONNX Runtime. This approach minimizes data transmission to central servers, reducing latency and improving privacy and security. Edge deployment is popular in industries like autonomous vehicles, healthcare, and smart cities.\n\n### 1.3. Introduction to Containerization with Docker\n\nEnter Docker, the magical tour bus for your model, ensuring it arrives at each destination ready to rock without missing a beat. Containerization has revolutionized the way applications are developed, shipped, and deployed, offering a standardized approach to packaging applications and their dependencies.\n\n<div style=\"text-align: center;\">\n  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190915151739/docker_flowchart-1024x640.png\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**What is Docker?**\nDocker is like your model's private dressing room, encapsulating all its dependencies, libraries, and code into a neat package called a \"container.\" This ensures that wherever your model goes, it performs consistently. Containers are lightweight, portable, and can run on any machine that supports Docker, eliminating the \"works on my machine\" problem.\n\nDocker containers are built from images, which are read-only templates describing how to create a container. These images can be shared and versioned, making it easy to distribute and update your application.\n\n**Why Docker?**\n\n- **Portability:** Containers are like your model's passport, allowing it to travel seamlessly across different environments. Docker ensures your application runs the same way on a developer's laptop, a test server, or a production cloud environment.\n- **Consistency:** Just as a band needs the same instruments wherever they play, Docker ensures your model has everything it needs. Containers include all dependencies, libraries, and configuration files required to run an application, ensuring consistency across deployments.\n- **Efficiency:** Docker containers are lightweight, meaning you can run multiple containers on a single machine without hogging resources. Unlike traditional virtual machines, containers share the host system's kernel, reducing overhead and improving resource utilization.\n- **Isolation:** Just as each band member needs their space, Docker provides isolated environments, ensuring no conflicts between models. Containers run in their own isolated environment, preventing interference from other applications or processes on the host system.\n\n**Building a Docker Image:**\nTo create a Docker image, you need to write a `Dockerfile`, which is a plain text file containing instructions on how to build the image. Here's a simple example of a `Dockerfile` for a Python application:\n\n```dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n```\n\nThis `Dockerfile` specifies a base image (`python:3.8-slim`), sets up a working directory, copies the application code, installs dependencies, exposes a port, and defines the command to run the application.\n\n### 1.4. Benefits of Using Docker for Deployment\n\nUsing Docker for deployment is akin to having a trusted roadie crew—everything runs smoother, faster, and with less hassle. Docker offers several benefits that make it an ideal choice for deploying machine learning models/\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fblockchainsimplified.com%2Fblog%2Fhow-docker-revolutionized-software-containerization%2F&psig=AOvVaw3Ua5CNeMweZOplg1eYtJdA&ust=1729003474615000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCJDNx4OOjokDFQAAAAAdAAAAABAJ\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Key Benefits:**\n\n- **Isolation:** Just as each band member needs their space, Docker provides isolated environments, ensuring no conflicts between models. Containers run in their own sandbox, separated from the host system and other containers, preventing interference from other applications or processes. This isolation also **enhances security by limiting the attack surface and potential vulnerabilities.**\n\n- **Reproducibility:** Docker guarantees that *\"what happens in rehearsal stays in rehearsal,\"* meaning your model will work anywhere it’s deployed. This reproducibility is crucial for machine learning workflows, as it ensures that the same environment used during development and testing is preserved in production. By encapsulating all dependencies and configurations in a Docker image, you can easily share and replicate your setup across different machines or teams.\n\n- **Scalability:** When your model's performance becomes a hit, Docker allows you to scale up effortlessly, ensuring it reaches audiences far and wide. Docker containers can be deployed and managed using orchestration tools like Kubernetes, which automates the scaling, distribution, and management of containerized applications. This enables you to efficiently handle varying workloads by adding or removing containers based on demand.\n\n- **Simplified Management:** Docker streamlines the deployment process, letting you focus on the music (or in this case, the model). With Docker, you can use a single command to build, ship, and run your application, simplifying the deployment pipeline and reducing the risk of errors. Additionally, Docker Hub, a cloud-based registry service, allows you to store and distribute your images, making it easy to manage updates and versioning.\n\n- **Resource Efficiency:** Docker containers are lightweight and share the host system's kernel, allowing you to run multiple containers on a single machine without incurring the overhead of traditional virtual machines. This efficient use of resources reduces infrastructure costs and improves performance, especially in environments with limited hardware capacity.\n\n- **Cross-Platform Compatibility:** Docker containers can run on any system that supports Docker, regardless of the underlying operating system. This cross-platform compatibility ensures that your application behaves consistently across different environments, from local development machines to cloud servers and edge devices.\n\n**Real-World Applications of Docker:**\n\nDocker is widely used across various industries to streamline the deployment and management of machine learning models. Here are a few examples:\n\n1. **FinTech:** Financial technology companies use Docker to deploy models for fraud detection, risk assessment, and automated trading. Docker's isolation and security features ensure that sensitive data and algorithms are protected, while its scalability allows for real-time processing of large volumes of financial transactions.\n\n2. **Healthcare:** In healthcare, Docker enables the deployment of models for medical imaging analysis, predictive diagnostics, and personalized treatment recommendations. By using containers, healthcare providers can ensure compliance with data privacy regulations and easily share models across different hospitals and research institutions.\n\n3. **E-commerce:** E-commerce platforms leverage Docker to deploy recommendation engines, customer segmentation models, and demand forecasting algorithms. Docker's resource efficiency and scalability allow these platforms to handle peak traffic during sales events and provide personalized experiences to millions of users.\n\n4. **Autonomous Vehicles:** The automotive industry uses Docker to deploy machine learning models for autonomous driving, object detection, and route planning. Docker's portability and cross-platform compatibility facilitate testing and deployment across various hardware and software configurations, accelerating the development of self-driving technologies.\n\n## 2. Getting Started with Docker\n\n### 2.1. Installing and Configuring Docker\n\nBefore we can start containerizing applications, we need to install Docker on our system. \n\nDocker provides a seamless installation process across various platforms, including Windows, macOS, and Linux.\n\nHere's a comprehensive guide on installing Docker across different platforms and ensuring it's ready for containerizing applications:\n\n### Installing Docker\n\nBefore diving into containerization, you need to install Docker on your system. Docker offers a straightforward installation process across various operating systems, including Windows, macOS, and Linux.\n\n### Installation Steps\n\n#### 1. **Docker Desktop (Windows & macOS)**\n\nFor users on Windows and macOS, **Docker Desktop** is the most convenient option. It provides a graphical user interface (GUI) and integrates seamlessly with your system for efficient container management.\n\n- **Download**: Visit the [official Docker website](https://docs.docker.com/desktop/install/windows-install/) to download Docker Desktop.\n- **Installation**:\n  - Follow the installation instructions provided on the website.\n  - During the installation, you may need to enable WSL 2 (Windows Subsystem for Linux) for Windows users, as this is required for Docker to run smoothly.\n\n#### 2. **Linux Installation**\n\nFor Linux users, Docker can be installed using package managers specific to your distribution.\n\n- **Ubuntu**:\n  ```bash\n  sudo apt update\n  sudo apt install apt-transport-https ca-certificates curl software-properties-common\n  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n  sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n  sudo apt update\n  sudo apt install docker-ce\n  ```\n\n- **CentOS**:\n  ```bash\n  sudo yum install -y yum-utils\n  sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n  sudo yum install docker-ce\n  ```\n\n- For detailed installation instructions for different Linux distributions, refer to the [official Docker documentation](https://docs.docker.com/engine/install/).\n\n#### 3. **Configuration**\n\nAfter installation, you may need to configure Docker to suit your development environment:\n\n- **Start Docker on Boot**: Ensure Docker starts automatically when your system boots up.\n- **Network Settings**: Configure any specific network settings as required for your applications.\n- **User Permissions**: Manage user permissions to allow specific users to run Docker commands without needing `sudo`.\n\nYou can find guidance on these configurations in the [Docker documentation](https://docs.docker.com/engine/install/).\n\n#### 4. **Verify Installation**\n\nOnce you've completed the installation, it’s important to verify that Docker is working correctly:\n\n- Open your terminal and run the following command:\n  ```bash\n  docker --version\n  ```\n- If Docker is installed properly, this command will display the installed Docker version, confirming that Docker is ready for use.\n\n### 2.2. Key Concepts: Images, Containers, Dockerfile\n\nUnderstanding Docker's core concepts is crucial for effectively using this powerful tool. Let's break down these concepts:\n\n**Images:**\nDocker images are the blueprints for containers. **They contain everything needed to run an application, including the code, runtime, libraries, and environment variables.** Images are built from a set of instructions defined in a `Dockerfile` and can be shared via Docker Hub or private registries.\n\n**Containers:**\nContainers are the running instances of Docker images. **They encapsulate an application and its environment, ensuring consistent behavior across different systems.** Containers are lightweight and can be started, stopped, and scaled independently, making them ideal for microservices architectures.\n\n**Dockerfile:**\nA `Dockerfile` is a text file containing a series of instructions on how to build a Docker image. **It specifies the base image, application code, dependencies, and any additional configuration needed.** Writing a `Dockerfile` is the first step in containerizing an application.\n\n### 2.3. Creating a Simple Docker Image for a FastAPI Application\n\nLet's create a simple Docker image for a FastAPI application. This hands-on example will guide you through the process of writing a `Dockerfile` and building an image.\n\n**Step-by-Step Guide:**\n\n1. **Create a FastAPI Application:**\nStart by creating a simple FastAPI application. For this example, we'll use a basic FastAPI app:\n\n```python\n# app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"Docker\"}\n```\n\n2. **Write a Dockerfile:**\nCreate a `Dockerfile` in the same directory as your FastAPI application. Here's a simple example:\n\n```dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install FastAPI and Uvicorn\nRUN pip install --no-cache-dir fastapi uvicorn\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Run app.py when the container launches\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```\n\n3. **Build the Docker Image:**\nOpen your terminal, navigate to the directory containing the `Dockerfile`, and run the following command to build the image:\n\n```bash\ndocker build -t my-fastapi-app .\n```\n\nThis command tells Docker to build an image named `my-fastapi-app` using the current directory as the build context.\n\n4. **Verify the Image:**\nOnce the build is complete, verify that the image was created successfully by running:\n\n```bash\ndocker images\n```\n\nYou should see `my-fastapi-app` listed among the available images.\n\n### 2.4. Running and Managing Docker Containers\n\nNow that we have a Docker image, let's run it as a container and explore how to manage it.\n\n**Running a Container:**\n\n1. **Start the Container:**\nUse the `docker run` command to start a container from the image we just built:\n\n```bash\ndocker run -d -p 4000:80 my-fastapi-app\n```\n\nThis command runs the container in detached mode (`-d`) and maps port 4000 on your host to port 80 in the container, allowing you to access the application via `http://localhost:4000`.\n\n2. **Verify the Container:**\nCheck that the container is running by listing all active containers:\n\n```bash\ndocker ps\n```\n\nYou should see `my-fastapi-app` in the list, along with its container ID and status.\n\n**Managing Containers:**\n\n1. **Stopping a Container:**\nTo stop a running container, use the `docker stop` command followed by the container ID or name:\n\n```bash\ndocker stop <container_id>\n```\n\n2. **Removing a Container:**\nOnce a container is stopped, you can remove it using the `docker rm` command:\n\n```bash\ndocker rm <container_id>\n```\n\n3. **Viewing Logs:**\nTo view the logs of a running container, use the `docker logs` command:\n\n```bash\ndocker logs <container_id>\n```\nThis is useful for debugging and monitoring your application's output.\n\n4. **Accessing a Container's Shell:**\nIf you need to interact with a container's file system or execute commands inside it, use the `docker exec` command to open a shell session:\n\n```bash\ndocker exec -it <container_id> /bin/bash\n```\n\n## 3. Dockerizing a Machine Learning Application\n\n#### 3.1 Introduction to the Example Machine Learning Application (Boston Housing Pricing)\n\nImagine you're a real estate wizard 🧙, and you have a magical tool that can predict house prices in Boston based on various features.\n\nThat's exactly what we're building—a machine learning model that predicts housing prices. But we're not stopping there! We'll wrap this model in a FastAPI application so others can use it to make predictions. \n\nThink of it as building a crystal ball 🔮 that anyone can access via the internet!\n\n#### 3.1.1  About the Boston Housing Dataset\n\n- **Dataset**: Contains information about Boston house prices, such as crime rate, number of rooms, and distance to employment centers.\n- **Goal**: Predict the median value of owner-occupied homes (in $1000's) based on these features.\n\n<div style=\"text-align: center;\">\n  <img src=\"\nhttps://miro.medium.com/v2/resize:fit:1000/1*FHQOSHMMT07CbXpklk1Ehw.jpeg\n\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Project Structure**\n\nLet's outline the structure of our project:\n\n```bash\nboston_housing_app/\n├── app.py\n├── model.joblib\n├── requirements.txt\n├── Dockerfile\n└── README.md\n```\n\n#### 3.1.2 Training the Machine Learning Model\n\nFirst, we'll train our model and save it for later use in our FastAPI app.\n\n```python\n# train_model.py\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport joblib\n\n# Load the dataset\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Save the model\njoblib.dump(model, 'model.joblib')\nprint(\"Model trained and saved!\")\n```\n\n**🔥Explanation**: We load the dataset, split it, train a Linear Regression model, and save it using `joblib`.\n\n**Run the Training Script**\n\n```bash\npython train_model.py\n```\n\nAfter running this script, you'll have a `model.joblib` file saved in your directory.\n\n---\n\n#### 3.1.3 Building the FastAPI Application\n\nNow, let's create a FastAPI application that loads this model and provides an endpoint to make predictions.\n\nAdd `fastapi` and `uvicorn` to your `requirements.txt`:\n\n```txt\n# requirements.txt\nfastapi\nuvicorn[standard]\npandas\nscikit-learn\njoblib\n```\n\n```python\n# app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\nimport pandas as pd\n\n# Initialize the app\napp = FastAPI(title=\"Boston Housing Price Prediction API\")\n\n# Load the model\nmodel = joblib.load('model.joblib')\n\n# Define the request body\nclass PredictionRequest(BaseModel):\n    CRIM: float\n    ZN: float\n    INDUS: float\n    CHAS: float\n    NOX: float\n    RM: float\n    AGE: float\n    DIS: float\n    RAD: float\n    TAX: float\n    PTRATIO: float\n    B: float\n    LSTAT: float\n\n# Define the response\nclass PredictionResponse(BaseModel):\n    price: float\n\n# Root path\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Welcome to the Boston Housing Price Prediction API!\"}\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(request: PredictionRequest):\n    data = request.dict()\n    df = pd.DataFrame([data])\n    prediction = model.predict(df)[0]\n    return {\"price\": prediction}\n```\n\n**🔥Explanation**:\n\n  - **FastAPI App**: We initialize a FastAPI app.\n  - **Model Loading**: We load the pre-trained model.\n  - **Request and Response Models**: Using Pydantic's `BaseModel` to define the expected input and output data structures.\n  - **Endpoints**:\n    - **GET `/`**: A welcome message.\n    - **POST `/predict`**: Accepts house features and returns the predicted price.\n\n**Running the FastAPI App Locally**\n\nTo run the app locally, you can use Uvicorn:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\nOpen your browser and navigate to `http://localhost:8000/docs` to see the interactive API docs provided by FastAPI! 🎉\n\n---\n\n### 3.2 Creating a Dockerfile for the Application\n\nNow that we have our FastAPI app ready, let's containerize it using Docker so we can deploy it anywhere.\n\n#### 3.2.1 Understanding the Dockerfile\n\nA `Dockerfile` is like a recipe for creating a Docker image. It tells Docker what base image to use, what files to copy, what commands to run, and which ports to expose.\n\n**Writing the Dockerfile**\n\n```dockerfile\n# Dockerfile\n\n# Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the code into the container\nCOPY . .\n\n# Expose port 8000\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n#### 3.2.2 Explanation of the Dockerfile\n\n- `FROM python:3.9-slim`: We start from a slim version of Python 3.9 to keep the image lightweight.\n- `WORKDIR /app`: Sets the working directory inside the container to `/app`.\n- `COPY requirements.txt .`: Copies `requirements.txt` into the container.\n- `RUN pip install --no-cache-dir -r requirements.txt`: Installs the dependencies without caching to save space.\n- `COPY . .`: Copies all files from the current directory into the container.\n- `EXPOSE 8000`: Exposes port `8000` for the app.\n- `CMD [...]`: The command to run when the container starts. Here, we start the Uvicorn server.\n\n---\n\n### 3.3 Defining Dependencies and Configurations\n\nDependencies are crucial. They ensure that our application has everything it needs to run correctly inside the container.\n\n#### 3.3.1 The `requirements.txt` File\n\nWe've already defined our dependencies in the `requirements.txt` file:\n\n```txt\nfastapi\nuvicorn[standard]\npandas\nscikit-learn\njoblib\n```\n\n#### 3.3.2 Environment Variables (Optional)\n\nIf your application requires environment variables (like API keys, database URLs), you can define them in the Dockerfile using `ENV` or pass them at runtime.\n\nExample in Dockerfile:\n\n```dockerfile\nENV MODEL_NAME=\"Boston Housing Predictor\"\n```\n\nIn `app.py`, you can access it:\n\n```python\nimport os\n\nmodel_name = os.getenv(\"MODEL_NAME\", \"Default Model\")\n```\n\n**But be cautious! For sensitive information like API keys, it's better to pass them at runtime or use Docker secrets.**\n\n---\n\n### 3.4 Exposing Ports and Setting the Run Command\n\nExposing ports is like opening the door 🚪 of your container to the outside world so that others can interact with your application.\n\n#### 3.4.1 Exposing Ports\n\nIn the Dockerfile:\n\n```dockerfile\nEXPOSE 8000\n```\n\nThis tells Docker that the container listens on port 8000 during runtime.\n\n#### 3.4.2 Setting the Run Command\n\nThe `CMD` instruction specifies the command to run when the container starts:\n\n```dockerfile\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n- `uvicorn app:app`: Runs the `app` object from `app.py` using Uvicorn.\n- `--host 0.0.0.0`: Makes the server accessible externally.\n- `--port 8000`: Runs the server on port 8000.\n\n---\n\n### 3.5 Building and Running the Docker Container\n\nTime to bring everything together and see the magic happen! 🎩✨\n\n#### 3.5.1 Build the Docker Image\n\nIn your terminal, navigate to the project directory and run:\n\n```bash\ndocker build -t boston-housing-api .\n```\n\n- `docker build`: Builds an image from a Dockerfile.\n- `-t boston-housing-api`: Tags the image with the name `boston-housing-api`.\n- `.`: Specifies the build context (current directory).\n\n#### 3.5.2 Run the Docker Container\n\n```bash\ndocker run -d -p 8000:8000 boston-housing-api\n```\n\n- `-d`: Runs the container in detached mode (in the background).\n- `-p 8000:8000`: Maps port 8000 of your local machine to port 8000 of the container.\n- `boston-housing-api`: The name of the image to run.\n\n#### 3.5.3 Verify the Application is Running\n\nOpen your browser and go to `http://localhost:8000/docs`. You should see the interactive API documentation.\n\n- Click on the `/predict` endpoint.\n- Click **Try it out**.\n- Enter sample data. For example:\n\n```json\n{\n  \"CRIM\": 0.1,\n  \"ZN\": 18.0,\n  \"INDUS\": 2.31,\n  \"CHAS\": 0.0,\n  \"NOX\": 0.538,\n  \"RM\": 6.575,\n  \"AGE\": 65.2,\n  \"DIS\": 4.09,\n  \"RAD\": 1.0,\n  \"TAX\": 296.0,\n  \"PTRATIO\": 15.3,\n  \"B\": 396.9,\n  \"LSTAT\": 4.98\n}\n```\n\n- Click **Execute**.\n- You should receive a predicted price in the response!\n\n---\n\n### 3.6 Deploying the Docker Image\n\nNow that our application is containerized, we can deploy it anywhere Docker is supported—be it a cloud service like AWS, Azure, Google Cloud, or even on a Raspberry Pi! Let's go over a simple deployment to a cloud service.\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.developpez.net/forums/attachments/p624417d1/a/a/a\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n### 3.6.1 Deploy to Heroku (Using Container Registry)\n\nHeroku is a cloud platform that supports Docker deployments via its Container Registry.\n\n**Prerequisites**:\n\n- Install the [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli).\n- Create a Heroku account.\n\n**Steps**:\n\n1. **Login to Heroku Container Registry**:\n\n   ```bash\n   heroku login\n   heroku container:login\n   ```\n\n2. **Create a New Heroku App**:\n\n   ```bash\n   heroku create boston-housing-api-app\n   ```\n\n3. **Push the Docker Image to Heroku**:\n\n   ```bash\n   heroku container:push web -a boston-housing-api-app\n   ```\n\n4. **Release the Image**:\n\n   ```bash\n   heroku container:release web -a boston-housing-api-app\n   ```\n\n5. **Open the App**:\n\n   ```bash\n   heroku open -a boston-housing-api-app\n   ```\n\nNow your API is live on the internet! 🌐\n\n### 3.6.2 Deploy to AWS Elastic Container Service (ECS)\n\nDeploying to AWS ECS involves more steps but offers robust scalability.\n\n**High-Level Steps**:\n\n- **Create a Docker Repository** in Amazon Elastic Container Registry (ECR).\n- **Push Your Docker Image** to ECR.\n- **Create a Task Definition** in ECS using your image.\n- **Run a Service** with the task definition.\n- **Set Up a Load Balancer** to route traffic to your service.\n\nDue to the complexity, consider following AWS's detailed documentation or use AWS Fargate for serverless container deployment.\n\n### Recap and Project Structure\n\nLet's revisit our project structure:\n\n```\nboston_housing_app/\n├── app.py\n├── model.joblib\n├── requirements.txt\n├── Dockerfile\n├── train_model.py\n└── README.md\n```\n\n- **app.py**: The FastAPI application.\n- **model.joblib**: The saved machine learning model.\n- **requirements.txt**: Lists all Python dependencies.\n- **Dockerfile**: Instructions to build the Docker image.\n- **train_model.py**: Script to train and save the model.\n- **README.md**: Documentation for your project.\n\n## 4. Introduction to GitHub Actions\n\nThe goal of this section is to introduce you to the powerful world of **Continuous Integration (CI)** and **Continuous Delivery (CD)**. By the end of this chapter, you'll have a fully automated pipeline, pushing your FastAPI app from your codebase straight to production (we’ll use **Heroku** as an example deployment platform).\n\nBut don’t let the technical jargon scare you—GitHub Actions is just like setting up a bunch of automated robot assistants to take care of the nitty-gritty, so you can focus on coding cool stuff!\n\n### 4.1 Principles of Continuous Integration and Continuous Delivery (CI/CD)\n\nLet’s start with the big picture: **CI/CD**. \n\nThese are the magic words behind modern software development. \n\nIt’s what allows big companies like Google and Netflix to deploy thousands of changes every day. So, what are they?\n\n#### 4.1.1 Continuous Integration (CI)\n\n**Think of CI as your safety net.**\n\nIt’s the practice of automatically testing and integrating small changes into your codebase. \n\nImagine you’re writing your FastAPI app and every time you push your code to GitHub, all your tests automatically run. \n\nIf something breaks, you get notified instantly, instead of finding out when the app is already deployed (which we all know is a nightmare).\n\n**Key Benefits of CI:**\n\n- **Instant feedback**: CI helps catch bugs early.\n- **Stable codebase**: Your main branch is always deployable.\n- **Developer collaboration**: Multiple people can work on the same codebase without conflicts.\n\n#### 4.1.2 Continuous Delivery (CD)\n\nCD is the natural extension of CI. It automates the release process so that your application is always in a deployable state. With CD, once your code passes the tests, it’s automatically pushed to a staging or production environment—without any manual steps.\n\n**Key Benefits of CD:**\n\n- **Frequent releases**: You can deploy to production multiple times a day.\n- **Fewer bugs**: Smaller, more frequent releases mean less complexity.\n- **Improved confidence**: Developers are less afraid of deploying code since it's automated and tested.\n\n\n### 4.2 Overview of GitHub Actions and Its Components\n\nNow that you’re familiar with CI/CD, let’s talk about **GitHub Actions**—your tool to automate everything from running tests to deploying applications. GitHub Actions are workflows that are triggered by events like a pull request, a new commit, or even a scheduled time.\n\n**Key Components of GitHub Actions:\n\n- **Workflow**: A series of actions (tasks) defined in YAML format that runs when a specific event occurs.\n- **Event**: The trigger that starts the workflow (e.g., a push to the repository, a pull request).\n- **Job**: A workflow contains one or more jobs. A job contains multiple steps and runs on its own virtual machine or container.\n- **Step**: A step can be a shell command, a script, or a reusable action. Multiple steps make up a job.\n- **Action**: A predefined task that can be used in steps (e.g., `actions/checkout@v2` checks out your code).\n\nThink of GitHub Actions as your very own robot assistants (like WALL-E) that automatically clean up after you every time you make a mess (push code). Each assistant (job) has its own task (test the app, create a Docker image, deploy it), and they all report back to you when their tasks are done.\n\n\n### 4.3 Creating a Basic GitHub Actions Workflow\n\nLet’s dive into the fun part—creating our first **CI pipeline** using GitHub Actions. We’ll start by setting up a workflow that runs tests on our FastAPI app whenever we push changes to GitHub.\n\n#### 4.3.1 Step 1: Creating the Workflow File\nYou’ll need to create a file in your repository at `.github/workflows/ci.yml`. GitHub will automatically detect this file and run the instructions inside whenever the specified events occur.\n\nHere’s a simple workflow that:\n- Triggers on every `push` and `pull_request` to the `main` branch.\n- Runs a set of Python unit tests.\n\n```yaml\nname: CI for FastAPI Application\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run tests\n        run: |\n          pytest\n```\n#### 4.3.2 Breakdown of Workflow\n\n- **on**: This defines when the workflow will be triggered. In our case, it will trigger on a `push` or a `pull_request` to the `main` branch.\n- **jobs**: Defines what jobs will run. Here we have a `test` job that runs on `ubuntu-latest` (a virtual machine provided by GitHub).\n- **steps**: Steps are the individual tasks for each job. In this case, we:\n  1. **Checkout the code** using `actions/checkout@v2`.\n  2. **Set up Python 3.9** using `actions/setup-python@v2`.\n  3. **Install dependencies** from the `requirements.txt` file.\n  4. **Run tests** using `pytest`.\n\nThis is your basic CI pipeline. Each time you push code, it automatically runs tests, letting you know if anything is broken before you deploy. Easy-peasy!\n\n### 4.4 Defining Triggers and Jobs for Deployment\n\nNow, let’s go a step further. Testing is important, but what if you could deploy your app every time your tests pass? Enter **CD**.\n\nWe’ll now define a trigger that not only runs tests but also deploys our FastAPI app.\n\nHere’s how you do it:\n\n```yaml\nname: CI/CD for FastAPI Application\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run tests\n        run: |\n          pytest\n\n  deploy:\n    runs-on: ubuntu-latest\n    needs: test  # Ensures deploy only runs if the tests pass\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Deploy to Heroku\n        run: |\n          heroku login\n          git push heroku main\n```\n\n**🔥Explanation**:\n\n- **deploy job**: Runs after the `test` job (`needs: test` ensures that deployment only happens if tests pass).\n- **Deploy to Heroku**: Uses `git push heroku main` to deploy the application.\n\n\n### 4.5 Creating a Deployment Workflow for Heroku\n\nNow let’s build a dedicated deployment workflow for **Heroku** using GitHub Actions. We’ll assume you already have a **Heroku** account and a deployed FastAPI app.\n\n#### 4.5.1 Setup Heroku CLI\n\nBefore running the deployment commands, ensure you install the Heroku CLI:\n\n```yaml\n- name: Install Heroku CLI\n  run: curl https://cli-assets.heroku.com/install.sh | sh\n```\n\n#### 4.5.2 Authenticating Heroku in GitHub Actions\n\nYou’ll need to authenticate GitHub Actions to access your Heroku app. For this, we’ll use **Heroku API keys** (don’t worry, we’ll cover how to keep these secure in the next section).\n\n```yaml\n- name: Authenticate Heroku\n  env:\n    HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n  run: |\n    echo \"machine api.heroku.com\" > ~/.netrc\n    echo \"  login ${{ secrets.HEROKU_EMAIL }}\" >> ~/.netrc\n    echo \"  password ${{ secrets.HEROKU_API_KEY }}\" >> ~/.netrc\n```\n\nThis authentication step uses the **Heroku API Key** and email (stored securely in **GitHub Secrets**—more on this soon).\n\n#### 4.5.3  Deploy Your FastAPI App\n\nThe final step is to deploy your app with Heroku:\n\n```yaml\n- name: Deploy to Heroku\n  run: git push heroku main\n```\n\n### 4.6 Using Secrets for Sensitive Information\n\nWe’ve mentioned **GitHub Secrets**, which is how we securely store sensitive information like API keys, credentials, or access tokens.\n\n- Go to your repository on GitHub.\n- Navigate to **Settings** -> **Secrets** -> **Actions**.\n- Add the following secrets:\n  - **HEROKU_API_KEY**: Your Heroku API key.\n  - **HEROKU_EMAIL**: The email associated with your Heroku account.\n\nNow, your workflow can use these secrets securely by referencing them as secrets.HEROKU_API_KEY and secrets.HEROKU_EMAIL.\n\n## 5. Advanced Tools and Technologies\n\nWelcome to the final chapter of your FastAPI journey! \n\nAt this point, you’ve learned how to build, containerize, and deploy your machine learning app using Docker, GitHub Actions, and Heroku. Now, let’s explore the next level of deployment tools and technologies. \n\nThis is where you unlock the door to **scalability**, **flexibility**, and **enterprise-grade cloud infrastructure**.\n\n### 5.1 Exploring Other Cloud Platforms for Deployment (AWS, GCP, Azure)\n\nIn the previous section, we deployed our FastAPI app to **Heroku**—a popular platform for fast deployments. But, as your app grows, you might need more flexibility and control over the infrastructure. That’s where the big players—**AWS**, **GCP**, and **Azure**—come into play. These platforms offer a wide range of services tailored for enterprise applications.\n\n#### AWS (Amazon Web Services)\n\nAmazon Web Services is the largest cloud provider in the world. AWS has **Elastic Beanstalk**, which simplifies deploying FastAPI apps. It abstracts much of the underlying infrastructure, but if you need full control, you can use **EC2** instances, **S3** for storage, and **RDS** for databases.\n\nHere’s how you can deploy a FastAPI app using AWS Elastic Beanstalk:\n\n1. **Install the AWS CLI**:\n   ```bash\n   pip install awsebcli\n   ```\n2. **Initialize Your Application**:\n   Inside your project directory:\n   ```bash\n   eb init -p python-3.8 fastapi-app --region <your-region>\n   ```\n3. **Create and Deploy the Application**:\n   ```bash\n   eb create fastapi-env\n   eb deploy\n   ```\n\nAWS gives you deep control over the configuration, security, and scaling of your application, which is perfect for enterprise-scale apps.\n\n#### Google Cloud Platform (GCP)\n\nGCP offers **App Engine**, **Compute Engine**, and **Cloud Run** for deploying FastAPI applications. **App Engine** is the easiest way to deploy apps without managing servers, while **Cloud Run** allows you to deploy containerized applications.\n\nDeploying your FastAPI app using **Google App Engine**:\n\n1. **Install the Google Cloud SDK**:\n   ```bash\n   curl https://sdk.cloud.google.com | bash\n   ```\n2. **Create the App Engine Configuration File (`app.yaml`)**:\n   ```yaml\n   runtime: python38\n   entrypoint: uvicorn main:app --host 0.0.0.0 --port $PORT\n   ```\n3. **Deploy the Application**:\n   ```bash\n   gcloud app deploy\n   ```\n\nGCP is known for its powerful machine learning services and data analytics tools, making it a great choice for apps that require heavy data processing.\n\n#### Azure\n\n**Azure App Service** and **Azure Kubernetes Service (AKS)** are the primary deployment platforms in the Microsoft cloud ecosystem. **Azure App Service** simplifies the deployment process while **AKS** offers enterprise-grade scalability for containerized applications.\n\nSteps to deploy using **Azure App Service**:\n\n1. **Install the Azure CLI**:\n   ```bash\n   az login\n   ```\n2. **Create an App Service Plan and Web App**:\n   ```bash\n   az webapp up --runtime \"PYTHON:3.8\" --name <app-name>\n   ```\n3. **Deploy the Application**:\n   Simply push your changes to the web app using Git or the Azure CLI.\n\n**Azure** offers deep integration with Microsoft's other tools and services, which is useful if you’re working in an enterprise environment already using Microsoft products.\n\n### 5.2 Using Container Orchestration Tools like Kubernetes\n\nDeploying individual Docker containers is great, but what if you want to scale up? That’s where **Kubernetes** comes into play. **Kubernetes** is the king of container orchestration. It helps you manage, scale, and maintain containerized applications across multiple servers (nodes).\n\nImagine Kubernetes as the traffic manager of your container city. It ensures that all traffic goes to the right containers (pods), scales the number of containers up or down based on demand, and keeps everything running smoothly.\n\n#### Why Use Kubernetes?\n\n- **Scalability**: Kubernetes automatically scales your application based on the number of requests.\n- **Self-Healing**: If one of your containers crashes, Kubernetes automatically restarts it.\n- **Load Balancing**: Kubernetes balances traffic across your containers so no one pod is overwhelmed.\n- **Deployment Rollbacks**: You can easily roll back to a previous version if something goes wrong.\n\n#### Deploying FastAPI on Kubernetes\n\nHere’s a basic overview of how to get your FastAPI app running on **Kubernetes**:\n\n1. **Create a Docker Image** for your FastAPI app (we’ve done this earlier).\n2. **Create a Kubernetes Deployment**:\n   ```yaml\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: fastapi-app\n   spec:\n     replicas: 3\n     selector:\n       matchLabels:\n         app: fastapi\n     template:\n       metadata:\n         labels:\n           app: fastapi\n       spec:\n         containers:\n         - name: fastapi-container\n           image: <your-docker-image>\n           ports:\n           - containerPort: 80\n   ```\n\n3. **Create a Service** to expose your app to the internet:\n   ```yaml\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: fastapi-service\n   spec:\n     selector:\n       app: fastapi\n     ports:\n     - protocol: TCP\n       port: 80\n       targetPort: 80\n     type: LoadBalancer\n   ```\n\n4. **Deploy on Kubernetes**:\n   ```bash\n   kubectl apply -f deployment.yaml\n   kubectl apply -f service.yaml\n   ```\n\nYou can deploy Kubernetes clusters on **AWS (EKS)**, **GCP (GKE)**, or **Azure (AKS)**, giving you the power to scale across the cloud.\n\n### 5.3 Integrating with Managed Machine Learning Services\n\nAs data professionals, one of the coolest things you can do is integrate your FastAPI app with **managed machine learning services**. These cloud services take care of the heavy lifting, allowing you to scale and deploy machine learning models seamlessly.\n\n#### Why Use Managed ML Services?\n\n- **Simplified Infrastructure**: You don’t have to worry about setting up complex machine learning environments.\n- **Auto-Scaling**: Cloud providers automatically scale your ML models based on usage.\n- **Integrations**: These platforms come with tools for deploying, monitoring, and managing models in production.\n\nLet’s look at how you can integrate with the big three: AWS, GCP, and Azure.\n\n#### AWS SageMaker\n\n**SageMaker** is AWS’s fully managed machine learning service. You can build, train, and deploy ML models directly from SageMaker, and integrate the deployed model into your FastAPI app.\n\nSteps to integrate SageMaker with FastAPI:\n1. **Train and Deploy Your Model on SageMaker**.\n   ```python\n   import sagemaker\n   from sagemaker import get_execution_role\n\n   # Define the session and role\n   session = sagemaker.Session()\n   role = get_execution_role()\n\n   # Train a model\n   estimator = sagemaker.estimator.Estimator(...)\n   estimator.fit(...)\n\n   # Deploy the model\n   predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n   ```\n\n2. **Invoke the Model from FastAPI**:\n   You can invoke the deployed model in your FastAPI app using the SageMaker **Runtime API**:\n   ```python\n   import boto3\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       sagemaker_client = boto3.client('sagemaker-runtime')\n       response = sagemaker_client.invoke_endpoint(\n           EndpointName='<your-endpoint>',\n           ContentType='application/json',\n           Body=json.dumps(data.dict())\n       )\n       return response\n   ```\n\n#### Google AI Platform\n\nGCP’s **AI Platform** offers tools for training and serving machine learning models. You can train a model in **AI Platform** and deploy it to a managed endpoint.\n\n1. **Deploy Your Model** to AI Platform:\n   ```bash\n   gcloud ai-platform models create model_name\n   gcloud ai-platform versions create version_name \\\n       --model model_name \\\n       --origin gs://path-to-your-model\n   ```\n\n2. **Integrate with FastAPI**:\n   Use the GCP API to make predictions from your FastAPI app.\n   ```python\n   from google.cloud import aiplatform\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       client = aiplatform.gapic.PredictionServiceClient()\n       response = client.predict(endpoint='<your-endpoint>', ...)\n       return response\n   ```\n\n#### Azure Machine Learning (AML)\n\n**Azure Machine Learning (AML)** is another managed service that allows you to train and deploy ML models at scale.\n\n1. **Deploy Your Model** to Azure:\n   ```bash\n   az ml model deploy -n model_name -m model.pkl --ic inference_config.json --dc deployment_config.json\n   ```\n\n2. **Call the Model from FastAPI**:\n   ```python\n   import requests\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       response = requests.post('<your-aml-endpoint>', json=data.dict())\n       return response.json()\n   ```\n   \n## In Summary \n\nCongratulations on completing the course!🎉\n\nYou’ve just navigated through an exciting journey of deploying machine learning applications. \n\nHere’s a recap of what you’ve mastered:\n\n- **Containerization with Docker**: You learned how to package your applications into Docker containers, ensuring consistent and portable deployments across different environments.\n- **CI/CD Pipelines with GitHub Actions**: You explored the principles of Continuous Integration and Continuous Delivery (CI/CD), leveraging GitHub Actions to automate your deployment workflows and streamline your development process.\n- **Heroku Deployment**: You successfully deployed your applications to Heroku, making it simple to launch your projects into the cloud and manage them effortlessly.\n- **Best Practices for Deployment**: You discovered essential best practices to optimize, scale, and secure your deployments, ensuring that your applications perform well under pressure.\n- **Advanced Tools and Technologies**: You explored various cloud platforms like AWS, GCP, and Azure, delved into container orchestration with Kubernetes, and integrated with managed machine learning services to enhance your applications.\n\nWith this knowledge, you are now equipped to deploy robust, scalable machine learning applications and take on any challenge in the tech universe. The journey doesn’t end here; keep experimenting, learning, and pushing the boundaries of what’s possible! 🚀\n\nIt’s time to apply everything you’ve learned, take your applications to the next level, and impress the world with your data science skills. \n\nNow, let’s dive into the **[Lab 3](../01_Exercises/03_Lab.qmd)**","srcMarkdownNoYaml":"\n\n## TL;DR:\n\nIf you're short on time, tune into the cosmic highlights of our course in this audio summary:\n\n<div style=\"text-align: center;\">\n  <figure>  \n    <figcaption>Listen to the Audio Overview:</figcaption>  \n    <audio controls src=\"03_Podcast.wav\"></audio>  \n    <br>\n    <button class=\"download-btn\" onclick=\"window.location.href='03_Podcast.wav'\">Download audio</button>  \n  </figure>\n</div>\n\nIn this course, we embark on a stellar journey to master the deployment of machine learning applications:\n\n- **Containerization** with Docker: Your starship for consistent and portable deployments.\n- **CI/CD Pipelines** using GitHub Actions: Automate your deployment journey like a seasoned Jedi.\n- **Heroku Deployment**: Launch your applications into the cloud galaxy with ease.\n- **Best Practices**: Optimize, scale, and secure your deployments for interstellar performance.\n- **Advanced Tools**: Explore the vast universe of cloud platforms and orchestration tools.\n\n---\n\n**Your deployment is a starship.**\n\nEach component is a crucial part of your mission, from the engines (Docker) to the navigation system (GitHub Actions).\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.science-et-vie.com/wp-content/uploads/scienceetvie/2024/03/starship_vol_3_une.jpg\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Containerization** is like building a reliable starship that can travel across different galaxies (environments) without a hitch. **CI/CD Pipelines** are your autopilot, ensuring smooth and automated journeys through the deployment cosmos.\n\n**Heroku** is your launchpad, propelling your applications into the cloud with the grace of a Jedi starfighter. **Best Practices** are your navigational charts, guiding you through optimization, scaling, and security challenges.\n\n**Advanced Tools** are the hyperdrive enhancements, allowing you to explore new frontiers in cloud deployment and orchestration.\n\n> Are you ready to launch your machine learning applications into the cloud galaxy?\n\n**Let's get started on this epic adventure!**\n\n## 1. Introduction to Deploying Machine Learning Applications\n\n### 1.1. The Challenges of Deploying Machine Learning Models\n\nDeploying machine learning models can sometimes feel like trying to fit a square peg into a round hole. Unlike traditional software, these models are like your favorite rock band—dynamic, unpredictable, and requiring lots of fine-tuning for each performance.\n\nLet's explore these challenges in detail:\n\n**Environment Mismatch:**\nImagine your model is a Broadway star, rehearsed and fine-tuned in one environment, but on opening night, the stage looks completely different.\n\nThis mismatch is one of the most common issues faced during deployment. \n\nDifferent machines might have various operating systems, library versions, or hardware configurations, causing unexpected behavior when the model is deployed. This is akin to a musician arriving at a venue only to find their instruments are tuned differently or missing entirely.\n\nTo mitigate this, it's essential to maintain a consistent environment across development and production stages. This can be achieved through environment management tools like virtualenv for Python or using containerization technologies like Docker, which we will explore later.\n\n**Scalability Issues:**\nPicture a concert where more fans show up than the venue can handle. Your model should be ready to handle the crowd, scaling up or down as needed. Scalability is crucial, especially in cloud-based applications where the user base can grow unpredictably. \n\nWithout proper scalability, applications can suffer from slow response times or crashes under heavy load.\n\nTo address scalability, load testing tools like Apache JMeter or Locust can be used to simulate a range of traffic conditions. Additionally, cloud platforms like AWS, Azure, and Google Cloud offer auto-scaling features that automatically adjust resources based on demand.\n\n**Version Control:**\nLike a band releasing multiple albums, tracking different versions of your models is crucial to know which one hits the right note. Imagine if every time a band played, they had to remember which version of their song arrangement they were supposed to perform. Similarly, in machine learning, keeping track of model versions ensures that you can revert to previous versions if a new model does not perform as expected.\n\nTools like DVC (Data Version Control) and MLflow provide mechanisms to manage and version control models, datasets, and experiments, making it easier to reproduce results and manage model lifecycle.\n\n**Performance Optimization:**\nEnsuring your model performs efficiently is like ensuring the lead singer hits all the right notes during a live performance. Performance can be influenced by factors such as model complexity, data input size, and computational resources. Optimizing performance is crucial for providing a seamless user experience and can also reduce computational costs.\n\nTechniques such as quantization, pruning, and knowledge distillation can be applied to reduce model size and improve inference speed without significantly sacrificing accuracy. Profiling tools like TensorBoard and PyTorch's profiler can help identify performance bottlenecks in your model.\n\n### 1.2. Different Deployment Approaches\n\n**Deploying models isn't a one-size-fits-all approach.** \n\nIt's more like choosing between a solo acoustic set or a full-on rock concert. Each deployment strategy has its pros and cons, depending on the specific requirements and constraints of the application.\n\nLet’s explore these different paths in detail:\n\n**Local Deployment:**\nIt's like playing music for your friends in your garage. Quick and easy, but not scalable. Local deployment is suitable for testing and development purposes, allowing you to quickly iterate and debug models on your local machine. However, it lacks the scalability and reliability needed for production environments.\n\nFor local deployment, tools like Jupyter Notebooks or local Flask servers can be used to serve models and test their endpoints. This approach is ideal for prototyping and learning but not for handling production-scale traffic.\n\n**Server-Based Deployment:**\nImagine booking a local venue. More organized and can handle a decent crowd, but may require manual tuning when issues arise. Server-based deployment involves hosting your model on dedicated servers, either on-premises or in the cloud. This approach provides more control over the environment and resources.\n\nServers can be configured using web frameworks like Flask or Django in combination with WSGI servers like Gunicorn. This setup provides flexibility and control, but requires manual management of scalability, load balancing, and failover mechanisms.\n\n**Cloud Deployment:**\nNow, we're talking major festival level. Your model gets the flexibility and power of cloud resources, scaling up and down with ease. Cloud deployment is the go-to solution for applications requiring high availability and scalability. Cloud platforms like AWS SageMaker, Azure Machine Learning, and Google AI Platform offer managed services for deploying and scaling models effortlessly.\n\nWith cloud deployment, you can leverage features like auto-scaling, load balancing, and continuous integration/continuous deployment (CI/CD) pipelines to ensure your model is always available and up-to-date. Additionally, cloud providers offer a variety of instance types, allowing you to choose the best resources for your model's needs.\n\n**Edge Deployment:**\nThink of it as a secret pop-up concert. Models are deployed closer to where the action happens, reducing latency and bandwidth use. Edge deployment is ideal for applications requiring real-time inference or operating in environments with limited connectivity.\n\nEdge devices, such as IoT devices, smartphones, or embedded systems, can run machine learning models using frameworks like TensorFlow Lite or ONNX Runtime. This approach minimizes data transmission to central servers, reducing latency and improving privacy and security. Edge deployment is popular in industries like autonomous vehicles, healthcare, and smart cities.\n\n### 1.3. Introduction to Containerization with Docker\n\nEnter Docker, the magical tour bus for your model, ensuring it arrives at each destination ready to rock without missing a beat. Containerization has revolutionized the way applications are developed, shipped, and deployed, offering a standardized approach to packaging applications and their dependencies.\n\n<div style=\"text-align: center;\">\n  <img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190915151739/docker_flowchart-1024x640.png\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**What is Docker?**\nDocker is like your model's private dressing room, encapsulating all its dependencies, libraries, and code into a neat package called a \"container.\" This ensures that wherever your model goes, it performs consistently. Containers are lightweight, portable, and can run on any machine that supports Docker, eliminating the \"works on my machine\" problem.\n\nDocker containers are built from images, which are read-only templates describing how to create a container. These images can be shared and versioned, making it easy to distribute and update your application.\n\n**Why Docker?**\n\n- **Portability:** Containers are like your model's passport, allowing it to travel seamlessly across different environments. Docker ensures your application runs the same way on a developer's laptop, a test server, or a production cloud environment.\n- **Consistency:** Just as a band needs the same instruments wherever they play, Docker ensures your model has everything it needs. Containers include all dependencies, libraries, and configuration files required to run an application, ensuring consistency across deployments.\n- **Efficiency:** Docker containers are lightweight, meaning you can run multiple containers on a single machine without hogging resources. Unlike traditional virtual machines, containers share the host system's kernel, reducing overhead and improving resource utilization.\n- **Isolation:** Just as each band member needs their space, Docker provides isolated environments, ensuring no conflicts between models. Containers run in their own isolated environment, preventing interference from other applications or processes on the host system.\n\n**Building a Docker Image:**\nTo create a Docker image, you need to write a `Dockerfile`, which is a plain text file containing instructions on how to build the image. Here's a simple example of a `Dockerfile` for a Python application:\n\n```dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n```\n\nThis `Dockerfile` specifies a base image (`python:3.8-slim`), sets up a working directory, copies the application code, installs dependencies, exposes a port, and defines the command to run the application.\n\n### 1.4. Benefits of Using Docker for Deployment\n\nUsing Docker for deployment is akin to having a trusted roadie crew—everything runs smoother, faster, and with less hassle. Docker offers several benefits that make it an ideal choice for deploying machine learning models/\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fblockchainsimplified.com%2Fblog%2Fhow-docker-revolutionized-software-containerization%2F&psig=AOvVaw3Ua5CNeMweZOplg1eYtJdA&ust=1729003474615000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCJDNx4OOjokDFQAAAAAdAAAAABAJ\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Key Benefits:**\n\n- **Isolation:** Just as each band member needs their space, Docker provides isolated environments, ensuring no conflicts between models. Containers run in their own sandbox, separated from the host system and other containers, preventing interference from other applications or processes. This isolation also **enhances security by limiting the attack surface and potential vulnerabilities.**\n\n- **Reproducibility:** Docker guarantees that *\"what happens in rehearsal stays in rehearsal,\"* meaning your model will work anywhere it’s deployed. This reproducibility is crucial for machine learning workflows, as it ensures that the same environment used during development and testing is preserved in production. By encapsulating all dependencies and configurations in a Docker image, you can easily share and replicate your setup across different machines or teams.\n\n- **Scalability:** When your model's performance becomes a hit, Docker allows you to scale up effortlessly, ensuring it reaches audiences far and wide. Docker containers can be deployed and managed using orchestration tools like Kubernetes, which automates the scaling, distribution, and management of containerized applications. This enables you to efficiently handle varying workloads by adding or removing containers based on demand.\n\n- **Simplified Management:** Docker streamlines the deployment process, letting you focus on the music (or in this case, the model). With Docker, you can use a single command to build, ship, and run your application, simplifying the deployment pipeline and reducing the risk of errors. Additionally, Docker Hub, a cloud-based registry service, allows you to store and distribute your images, making it easy to manage updates and versioning.\n\n- **Resource Efficiency:** Docker containers are lightweight and share the host system's kernel, allowing you to run multiple containers on a single machine without incurring the overhead of traditional virtual machines. This efficient use of resources reduces infrastructure costs and improves performance, especially in environments with limited hardware capacity.\n\n- **Cross-Platform Compatibility:** Docker containers can run on any system that supports Docker, regardless of the underlying operating system. This cross-platform compatibility ensures that your application behaves consistently across different environments, from local development machines to cloud servers and edge devices.\n\n**Real-World Applications of Docker:**\n\nDocker is widely used across various industries to streamline the deployment and management of machine learning models. Here are a few examples:\n\n1. **FinTech:** Financial technology companies use Docker to deploy models for fraud detection, risk assessment, and automated trading. Docker's isolation and security features ensure that sensitive data and algorithms are protected, while its scalability allows for real-time processing of large volumes of financial transactions.\n\n2. **Healthcare:** In healthcare, Docker enables the deployment of models for medical imaging analysis, predictive diagnostics, and personalized treatment recommendations. By using containers, healthcare providers can ensure compliance with data privacy regulations and easily share models across different hospitals and research institutions.\n\n3. **E-commerce:** E-commerce platforms leverage Docker to deploy recommendation engines, customer segmentation models, and demand forecasting algorithms. Docker's resource efficiency and scalability allow these platforms to handle peak traffic during sales events and provide personalized experiences to millions of users.\n\n4. **Autonomous Vehicles:** The automotive industry uses Docker to deploy machine learning models for autonomous driving, object detection, and route planning. Docker's portability and cross-platform compatibility facilitate testing and deployment across various hardware and software configurations, accelerating the development of self-driving technologies.\n\n## 2. Getting Started with Docker\n\n### 2.1. Installing and Configuring Docker\n\nBefore we can start containerizing applications, we need to install Docker on our system. \n\nDocker provides a seamless installation process across various platforms, including Windows, macOS, and Linux.\n\nHere's a comprehensive guide on installing Docker across different platforms and ensuring it's ready for containerizing applications:\n\n### Installing Docker\n\nBefore diving into containerization, you need to install Docker on your system. Docker offers a straightforward installation process across various operating systems, including Windows, macOS, and Linux.\n\n### Installation Steps\n\n#### 1. **Docker Desktop (Windows & macOS)**\n\nFor users on Windows and macOS, **Docker Desktop** is the most convenient option. It provides a graphical user interface (GUI) and integrates seamlessly with your system for efficient container management.\n\n- **Download**: Visit the [official Docker website](https://docs.docker.com/desktop/install/windows-install/) to download Docker Desktop.\n- **Installation**:\n  - Follow the installation instructions provided on the website.\n  - During the installation, you may need to enable WSL 2 (Windows Subsystem for Linux) for Windows users, as this is required for Docker to run smoothly.\n\n#### 2. **Linux Installation**\n\nFor Linux users, Docker can be installed using package managers specific to your distribution.\n\n- **Ubuntu**:\n  ```bash\n  sudo apt update\n  sudo apt install apt-transport-https ca-certificates curl software-properties-common\n  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n  sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n  sudo apt update\n  sudo apt install docker-ce\n  ```\n\n- **CentOS**:\n  ```bash\n  sudo yum install -y yum-utils\n  sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n  sudo yum install docker-ce\n  ```\n\n- For detailed installation instructions for different Linux distributions, refer to the [official Docker documentation](https://docs.docker.com/engine/install/).\n\n#### 3. **Configuration**\n\nAfter installation, you may need to configure Docker to suit your development environment:\n\n- **Start Docker on Boot**: Ensure Docker starts automatically when your system boots up.\n- **Network Settings**: Configure any specific network settings as required for your applications.\n- **User Permissions**: Manage user permissions to allow specific users to run Docker commands without needing `sudo`.\n\nYou can find guidance on these configurations in the [Docker documentation](https://docs.docker.com/engine/install/).\n\n#### 4. **Verify Installation**\n\nOnce you've completed the installation, it’s important to verify that Docker is working correctly:\n\n- Open your terminal and run the following command:\n  ```bash\n  docker --version\n  ```\n- If Docker is installed properly, this command will display the installed Docker version, confirming that Docker is ready for use.\n\n### 2.2. Key Concepts: Images, Containers, Dockerfile\n\nUnderstanding Docker's core concepts is crucial for effectively using this powerful tool. Let's break down these concepts:\n\n**Images:**\nDocker images are the blueprints for containers. **They contain everything needed to run an application, including the code, runtime, libraries, and environment variables.** Images are built from a set of instructions defined in a `Dockerfile` and can be shared via Docker Hub or private registries.\n\n**Containers:**\nContainers are the running instances of Docker images. **They encapsulate an application and its environment, ensuring consistent behavior across different systems.** Containers are lightweight and can be started, stopped, and scaled independently, making them ideal for microservices architectures.\n\n**Dockerfile:**\nA `Dockerfile` is a text file containing a series of instructions on how to build a Docker image. **It specifies the base image, application code, dependencies, and any additional configuration needed.** Writing a `Dockerfile` is the first step in containerizing an application.\n\n### 2.3. Creating a Simple Docker Image for a FastAPI Application\n\nLet's create a simple Docker image for a FastAPI application. This hands-on example will guide you through the process of writing a `Dockerfile` and building an image.\n\n**Step-by-Step Guide:**\n\n1. **Create a FastAPI Application:**\nStart by creating a simple FastAPI application. For this example, we'll use a basic FastAPI app:\n\n```python\n# app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"Docker\"}\n```\n\n2. **Write a Dockerfile:**\nCreate a `Dockerfile` in the same directory as your FastAPI application. Here's a simple example:\n\n```dockerfile\n# Use an official Python runtime as a parent image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install FastAPI and Uvicorn\nRUN pip install --no-cache-dir fastapi uvicorn\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Run app.py when the container launches\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n```\n\n3. **Build the Docker Image:**\nOpen your terminal, navigate to the directory containing the `Dockerfile`, and run the following command to build the image:\n\n```bash\ndocker build -t my-fastapi-app .\n```\n\nThis command tells Docker to build an image named `my-fastapi-app` using the current directory as the build context.\n\n4. **Verify the Image:**\nOnce the build is complete, verify that the image was created successfully by running:\n\n```bash\ndocker images\n```\n\nYou should see `my-fastapi-app` listed among the available images.\n\n### 2.4. Running and Managing Docker Containers\n\nNow that we have a Docker image, let's run it as a container and explore how to manage it.\n\n**Running a Container:**\n\n1. **Start the Container:**\nUse the `docker run` command to start a container from the image we just built:\n\n```bash\ndocker run -d -p 4000:80 my-fastapi-app\n```\n\nThis command runs the container in detached mode (`-d`) and maps port 4000 on your host to port 80 in the container, allowing you to access the application via `http://localhost:4000`.\n\n2. **Verify the Container:**\nCheck that the container is running by listing all active containers:\n\n```bash\ndocker ps\n```\n\nYou should see `my-fastapi-app` in the list, along with its container ID and status.\n\n**Managing Containers:**\n\n1. **Stopping a Container:**\nTo stop a running container, use the `docker stop` command followed by the container ID or name:\n\n```bash\ndocker stop <container_id>\n```\n\n2. **Removing a Container:**\nOnce a container is stopped, you can remove it using the `docker rm` command:\n\n```bash\ndocker rm <container_id>\n```\n\n3. **Viewing Logs:**\nTo view the logs of a running container, use the `docker logs` command:\n\n```bash\ndocker logs <container_id>\n```\nThis is useful for debugging and monitoring your application's output.\n\n4. **Accessing a Container's Shell:**\nIf you need to interact with a container's file system or execute commands inside it, use the `docker exec` command to open a shell session:\n\n```bash\ndocker exec -it <container_id> /bin/bash\n```\n\n## 3. Dockerizing a Machine Learning Application\n\n#### 3.1 Introduction to the Example Machine Learning Application (Boston Housing Pricing)\n\nImagine you're a real estate wizard 🧙, and you have a magical tool that can predict house prices in Boston based on various features.\n\nThat's exactly what we're building—a machine learning model that predicts housing prices. But we're not stopping there! We'll wrap this model in a FastAPI application so others can use it to make predictions. \n\nThink of it as building a crystal ball 🔮 that anyone can access via the internet!\n\n#### 3.1.1  About the Boston Housing Dataset\n\n- **Dataset**: Contains information about Boston house prices, such as crime rate, number of rooms, and distance to employment centers.\n- **Goal**: Predict the median value of owner-occupied homes (in $1000's) based on these features.\n\n<div style=\"text-align: center;\">\n  <img src=\"\nhttps://miro.medium.com/v2/resize:fit:1000/1*FHQOSHMMT07CbXpklk1Ehw.jpeg\n\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n**Project Structure**\n\nLet's outline the structure of our project:\n\n```bash\nboston_housing_app/\n├── app.py\n├── model.joblib\n├── requirements.txt\n├── Dockerfile\n└── README.md\n```\n\n#### 3.1.2 Training the Machine Learning Model\n\nFirst, we'll train our model and save it for later use in our FastAPI app.\n\n```python\n# train_model.py\nimport pandas as pd\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport joblib\n\n# Load the dataset\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Save the model\njoblib.dump(model, 'model.joblib')\nprint(\"Model trained and saved!\")\n```\n\n**🔥Explanation**: We load the dataset, split it, train a Linear Regression model, and save it using `joblib`.\n\n**Run the Training Script**\n\n```bash\npython train_model.py\n```\n\nAfter running this script, you'll have a `model.joblib` file saved in your directory.\n\n---\n\n#### 3.1.3 Building the FastAPI Application\n\nNow, let's create a FastAPI application that loads this model and provides an endpoint to make predictions.\n\nAdd `fastapi` and `uvicorn` to your `requirements.txt`:\n\n```txt\n# requirements.txt\nfastapi\nuvicorn[standard]\npandas\nscikit-learn\njoblib\n```\n\n```python\n# app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\nimport pandas as pd\n\n# Initialize the app\napp = FastAPI(title=\"Boston Housing Price Prediction API\")\n\n# Load the model\nmodel = joblib.load('model.joblib')\n\n# Define the request body\nclass PredictionRequest(BaseModel):\n    CRIM: float\n    ZN: float\n    INDUS: float\n    CHAS: float\n    NOX: float\n    RM: float\n    AGE: float\n    DIS: float\n    RAD: float\n    TAX: float\n    PTRATIO: float\n    B: float\n    LSTAT: float\n\n# Define the response\nclass PredictionResponse(BaseModel):\n    price: float\n\n# Root path\n@app.get(\"/\")\ndef read_root():\n    return {\"message\": \"Welcome to the Boston Housing Price Prediction API!\"}\n\n# Prediction endpoint\n@app.post(\"/predict\", response_model=PredictionResponse)\ndef predict(request: PredictionRequest):\n    data = request.dict()\n    df = pd.DataFrame([data])\n    prediction = model.predict(df)[0]\n    return {\"price\": prediction}\n```\n\n**🔥Explanation**:\n\n  - **FastAPI App**: We initialize a FastAPI app.\n  - **Model Loading**: We load the pre-trained model.\n  - **Request and Response Models**: Using Pydantic's `BaseModel` to define the expected input and output data structures.\n  - **Endpoints**:\n    - **GET `/`**: A welcome message.\n    - **POST `/predict`**: Accepts house features and returns the predicted price.\n\n**Running the FastAPI App Locally**\n\nTo run the app locally, you can use Uvicorn:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\nOpen your browser and navigate to `http://localhost:8000/docs` to see the interactive API docs provided by FastAPI! 🎉\n\n---\n\n### 3.2 Creating a Dockerfile for the Application\n\nNow that we have our FastAPI app ready, let's containerize it using Docker so we can deploy it anywhere.\n\n#### 3.2.1 Understanding the Dockerfile\n\nA `Dockerfile` is like a recipe for creating a Docker image. It tells Docker what base image to use, what files to copy, what commands to run, and which ports to expose.\n\n**Writing the Dockerfile**\n\n```dockerfile\n# Dockerfile\n\n# Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy the requirements file into the container\nCOPY requirements.txt .\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the code into the container\nCOPY . .\n\n# Expose port 8000\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n#### 3.2.2 Explanation of the Dockerfile\n\n- `FROM python:3.9-slim`: We start from a slim version of Python 3.9 to keep the image lightweight.\n- `WORKDIR /app`: Sets the working directory inside the container to `/app`.\n- `COPY requirements.txt .`: Copies `requirements.txt` into the container.\n- `RUN pip install --no-cache-dir -r requirements.txt`: Installs the dependencies without caching to save space.\n- `COPY . .`: Copies all files from the current directory into the container.\n- `EXPOSE 8000`: Exposes port `8000` for the app.\n- `CMD [...]`: The command to run when the container starts. Here, we start the Uvicorn server.\n\n---\n\n### 3.3 Defining Dependencies and Configurations\n\nDependencies are crucial. They ensure that our application has everything it needs to run correctly inside the container.\n\n#### 3.3.1 The `requirements.txt` File\n\nWe've already defined our dependencies in the `requirements.txt` file:\n\n```txt\nfastapi\nuvicorn[standard]\npandas\nscikit-learn\njoblib\n```\n\n#### 3.3.2 Environment Variables (Optional)\n\nIf your application requires environment variables (like API keys, database URLs), you can define them in the Dockerfile using `ENV` or pass them at runtime.\n\nExample in Dockerfile:\n\n```dockerfile\nENV MODEL_NAME=\"Boston Housing Predictor\"\n```\n\nIn `app.py`, you can access it:\n\n```python\nimport os\n\nmodel_name = os.getenv(\"MODEL_NAME\", \"Default Model\")\n```\n\n**But be cautious! For sensitive information like API keys, it's better to pass them at runtime or use Docker secrets.**\n\n---\n\n### 3.4 Exposing Ports and Setting the Run Command\n\nExposing ports is like opening the door 🚪 of your container to the outside world so that others can interact with your application.\n\n#### 3.4.1 Exposing Ports\n\nIn the Dockerfile:\n\n```dockerfile\nEXPOSE 8000\n```\n\nThis tells Docker that the container listens on port 8000 during runtime.\n\n#### 3.4.2 Setting the Run Command\n\nThe `CMD` instruction specifies the command to run when the container starts:\n\n```dockerfile\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n```\n\n- `uvicorn app:app`: Runs the `app` object from `app.py` using Uvicorn.\n- `--host 0.0.0.0`: Makes the server accessible externally.\n- `--port 8000`: Runs the server on port 8000.\n\n---\n\n### 3.5 Building and Running the Docker Container\n\nTime to bring everything together and see the magic happen! 🎩✨\n\n#### 3.5.1 Build the Docker Image\n\nIn your terminal, navigate to the project directory and run:\n\n```bash\ndocker build -t boston-housing-api .\n```\n\n- `docker build`: Builds an image from a Dockerfile.\n- `-t boston-housing-api`: Tags the image with the name `boston-housing-api`.\n- `.`: Specifies the build context (current directory).\n\n#### 3.5.2 Run the Docker Container\n\n```bash\ndocker run -d -p 8000:8000 boston-housing-api\n```\n\n- `-d`: Runs the container in detached mode (in the background).\n- `-p 8000:8000`: Maps port 8000 of your local machine to port 8000 of the container.\n- `boston-housing-api`: The name of the image to run.\n\n#### 3.5.3 Verify the Application is Running\n\nOpen your browser and go to `http://localhost:8000/docs`. You should see the interactive API documentation.\n\n- Click on the `/predict` endpoint.\n- Click **Try it out**.\n- Enter sample data. For example:\n\n```json\n{\n  \"CRIM\": 0.1,\n  \"ZN\": 18.0,\n  \"INDUS\": 2.31,\n  \"CHAS\": 0.0,\n  \"NOX\": 0.538,\n  \"RM\": 6.575,\n  \"AGE\": 65.2,\n  \"DIS\": 4.09,\n  \"RAD\": 1.0,\n  \"TAX\": 296.0,\n  \"PTRATIO\": 15.3,\n  \"B\": 396.9,\n  \"LSTAT\": 4.98\n}\n```\n\n- Click **Execute**.\n- You should receive a predicted price in the response!\n\n---\n\n### 3.6 Deploying the Docker Image\n\nNow that our application is containerized, we can deploy it anywhere Docker is supported—be it a cloud service like AWS, Azure, Google Cloud, or even on a Raspberry Pi! Let's go over a simple deployment to a cloud service.\n\n<div style=\"text-align: center;\">\n  <img src=\"https://www.developpez.net/forums/attachments/p624417d1/a/a/a\" alt=\"Starship\" style=\"width: 400px;\">\n</div>\n\n### 3.6.1 Deploy to Heroku (Using Container Registry)\n\nHeroku is a cloud platform that supports Docker deployments via its Container Registry.\n\n**Prerequisites**:\n\n- Install the [Heroku CLI](https://devcenter.heroku.com/articles/heroku-cli).\n- Create a Heroku account.\n\n**Steps**:\n\n1. **Login to Heroku Container Registry**:\n\n   ```bash\n   heroku login\n   heroku container:login\n   ```\n\n2. **Create a New Heroku App**:\n\n   ```bash\n   heroku create boston-housing-api-app\n   ```\n\n3. **Push the Docker Image to Heroku**:\n\n   ```bash\n   heroku container:push web -a boston-housing-api-app\n   ```\n\n4. **Release the Image**:\n\n   ```bash\n   heroku container:release web -a boston-housing-api-app\n   ```\n\n5. **Open the App**:\n\n   ```bash\n   heroku open -a boston-housing-api-app\n   ```\n\nNow your API is live on the internet! 🌐\n\n### 3.6.2 Deploy to AWS Elastic Container Service (ECS)\n\nDeploying to AWS ECS involves more steps but offers robust scalability.\n\n**High-Level Steps**:\n\n- **Create a Docker Repository** in Amazon Elastic Container Registry (ECR).\n- **Push Your Docker Image** to ECR.\n- **Create a Task Definition** in ECS using your image.\n- **Run a Service** with the task definition.\n- **Set Up a Load Balancer** to route traffic to your service.\n\nDue to the complexity, consider following AWS's detailed documentation or use AWS Fargate for serverless container deployment.\n\n### Recap and Project Structure\n\nLet's revisit our project structure:\n\n```\nboston_housing_app/\n├── app.py\n├── model.joblib\n├── requirements.txt\n├── Dockerfile\n├── train_model.py\n└── README.md\n```\n\n- **app.py**: The FastAPI application.\n- **model.joblib**: The saved machine learning model.\n- **requirements.txt**: Lists all Python dependencies.\n- **Dockerfile**: Instructions to build the Docker image.\n- **train_model.py**: Script to train and save the model.\n- **README.md**: Documentation for your project.\n\n## 4. Introduction to GitHub Actions\n\nThe goal of this section is to introduce you to the powerful world of **Continuous Integration (CI)** and **Continuous Delivery (CD)**. By the end of this chapter, you'll have a fully automated pipeline, pushing your FastAPI app from your codebase straight to production (we’ll use **Heroku** as an example deployment platform).\n\nBut don’t let the technical jargon scare you—GitHub Actions is just like setting up a bunch of automated robot assistants to take care of the nitty-gritty, so you can focus on coding cool stuff!\n\n### 4.1 Principles of Continuous Integration and Continuous Delivery (CI/CD)\n\nLet’s start with the big picture: **CI/CD**. \n\nThese are the magic words behind modern software development. \n\nIt’s what allows big companies like Google and Netflix to deploy thousands of changes every day. So, what are they?\n\n#### 4.1.1 Continuous Integration (CI)\n\n**Think of CI as your safety net.**\n\nIt’s the practice of automatically testing and integrating small changes into your codebase. \n\nImagine you’re writing your FastAPI app and every time you push your code to GitHub, all your tests automatically run. \n\nIf something breaks, you get notified instantly, instead of finding out when the app is already deployed (which we all know is a nightmare).\n\n**Key Benefits of CI:**\n\n- **Instant feedback**: CI helps catch bugs early.\n- **Stable codebase**: Your main branch is always deployable.\n- **Developer collaboration**: Multiple people can work on the same codebase without conflicts.\n\n#### 4.1.2 Continuous Delivery (CD)\n\nCD is the natural extension of CI. It automates the release process so that your application is always in a deployable state. With CD, once your code passes the tests, it’s automatically pushed to a staging or production environment—without any manual steps.\n\n**Key Benefits of CD:**\n\n- **Frequent releases**: You can deploy to production multiple times a day.\n- **Fewer bugs**: Smaller, more frequent releases mean less complexity.\n- **Improved confidence**: Developers are less afraid of deploying code since it's automated and tested.\n\n\n### 4.2 Overview of GitHub Actions and Its Components\n\nNow that you’re familiar with CI/CD, let’s talk about **GitHub Actions**—your tool to automate everything from running tests to deploying applications. GitHub Actions are workflows that are triggered by events like a pull request, a new commit, or even a scheduled time.\n\n**Key Components of GitHub Actions:\n\n- **Workflow**: A series of actions (tasks) defined in YAML format that runs when a specific event occurs.\n- **Event**: The trigger that starts the workflow (e.g., a push to the repository, a pull request).\n- **Job**: A workflow contains one or more jobs. A job contains multiple steps and runs on its own virtual machine or container.\n- **Step**: A step can be a shell command, a script, or a reusable action. Multiple steps make up a job.\n- **Action**: A predefined task that can be used in steps (e.g., `actions/checkout@v2` checks out your code).\n\nThink of GitHub Actions as your very own robot assistants (like WALL-E) that automatically clean up after you every time you make a mess (push code). Each assistant (job) has its own task (test the app, create a Docker image, deploy it), and they all report back to you when their tasks are done.\n\n\n### 4.3 Creating a Basic GitHub Actions Workflow\n\nLet’s dive into the fun part—creating our first **CI pipeline** using GitHub Actions. We’ll start by setting up a workflow that runs tests on our FastAPI app whenever we push changes to GitHub.\n\n#### 4.3.1 Step 1: Creating the Workflow File\nYou’ll need to create a file in your repository at `.github/workflows/ci.yml`. GitHub will automatically detect this file and run the instructions inside whenever the specified events occur.\n\nHere’s a simple workflow that:\n- Triggers on every `push` and `pull_request` to the `main` branch.\n- Runs a set of Python unit tests.\n\n```yaml\nname: CI for FastAPI Application\n\non:\n  push:\n    branches:\n      - main\n  pull_request:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run tests\n        run: |\n          pytest\n```\n#### 4.3.2 Breakdown of Workflow\n\n- **on**: This defines when the workflow will be triggered. In our case, it will trigger on a `push` or a `pull_request` to the `main` branch.\n- **jobs**: Defines what jobs will run. Here we have a `test` job that runs on `ubuntu-latest` (a virtual machine provided by GitHub).\n- **steps**: Steps are the individual tasks for each job. In this case, we:\n  1. **Checkout the code** using `actions/checkout@v2`.\n  2. **Set up Python 3.9** using `actions/setup-python@v2`.\n  3. **Install dependencies** from the `requirements.txt` file.\n  4. **Run tests** using `pytest`.\n\nThis is your basic CI pipeline. Each time you push code, it automatically runs tests, letting you know if anything is broken before you deploy. Easy-peasy!\n\n### 4.4 Defining Triggers and Jobs for Deployment\n\nNow, let’s go a step further. Testing is important, but what if you could deploy your app every time your tests pass? Enter **CD**.\n\nWe’ll now define a trigger that not only runs tests but also deploys our FastAPI app.\n\nHere’s how you do it:\n\n```yaml\nname: CI/CD for FastAPI Application\n\non:\n  push:\n    branches:\n      - main\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: '3.9'\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run tests\n        run: |\n          pytest\n\n  deploy:\n    runs-on: ubuntu-latest\n    needs: test  # Ensures deploy only runs if the tests pass\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v2\n\n      - name: Deploy to Heroku\n        run: |\n          heroku login\n          git push heroku main\n```\n\n**🔥Explanation**:\n\n- **deploy job**: Runs after the `test` job (`needs: test` ensures that deployment only happens if tests pass).\n- **Deploy to Heroku**: Uses `git push heroku main` to deploy the application.\n\n\n### 4.5 Creating a Deployment Workflow for Heroku\n\nNow let’s build a dedicated deployment workflow for **Heroku** using GitHub Actions. We’ll assume you already have a **Heroku** account and a deployed FastAPI app.\n\n#### 4.5.1 Setup Heroku CLI\n\nBefore running the deployment commands, ensure you install the Heroku CLI:\n\n```yaml\n- name: Install Heroku CLI\n  run: curl https://cli-assets.heroku.com/install.sh | sh\n```\n\n#### 4.5.2 Authenticating Heroku in GitHub Actions\n\nYou’ll need to authenticate GitHub Actions to access your Heroku app. For this, we’ll use **Heroku API keys** (don’t worry, we’ll cover how to keep these secure in the next section).\n\n```yaml\n- name: Authenticate Heroku\n  env:\n    HEROKU_API_KEY: ${{ secrets.HEROKU_API_KEY }}\n  run: |\n    echo \"machine api.heroku.com\" > ~/.netrc\n    echo \"  login ${{ secrets.HEROKU_EMAIL }}\" >> ~/.netrc\n    echo \"  password ${{ secrets.HEROKU_API_KEY }}\" >> ~/.netrc\n```\n\nThis authentication step uses the **Heroku API Key** and email (stored securely in **GitHub Secrets**—more on this soon).\n\n#### 4.5.3  Deploy Your FastAPI App\n\nThe final step is to deploy your app with Heroku:\n\n```yaml\n- name: Deploy to Heroku\n  run: git push heroku main\n```\n\n### 4.6 Using Secrets for Sensitive Information\n\nWe’ve mentioned **GitHub Secrets**, which is how we securely store sensitive information like API keys, credentials, or access tokens.\n\n- Go to your repository on GitHub.\n- Navigate to **Settings** -> **Secrets** -> **Actions**.\n- Add the following secrets:\n  - **HEROKU_API_KEY**: Your Heroku API key.\n  - **HEROKU_EMAIL**: The email associated with your Heroku account.\n\nNow, your workflow can use these secrets securely by referencing them as secrets.HEROKU_API_KEY and secrets.HEROKU_EMAIL.\n\n## 5. Advanced Tools and Technologies\n\nWelcome to the final chapter of your FastAPI journey! \n\nAt this point, you’ve learned how to build, containerize, and deploy your machine learning app using Docker, GitHub Actions, and Heroku. Now, let’s explore the next level of deployment tools and technologies. \n\nThis is where you unlock the door to **scalability**, **flexibility**, and **enterprise-grade cloud infrastructure**.\n\n### 5.1 Exploring Other Cloud Platforms for Deployment (AWS, GCP, Azure)\n\nIn the previous section, we deployed our FastAPI app to **Heroku**—a popular platform for fast deployments. But, as your app grows, you might need more flexibility and control over the infrastructure. That’s where the big players—**AWS**, **GCP**, and **Azure**—come into play. These platforms offer a wide range of services tailored for enterprise applications.\n\n#### AWS (Amazon Web Services)\n\nAmazon Web Services is the largest cloud provider in the world. AWS has **Elastic Beanstalk**, which simplifies deploying FastAPI apps. It abstracts much of the underlying infrastructure, but if you need full control, you can use **EC2** instances, **S3** for storage, and **RDS** for databases.\n\nHere’s how you can deploy a FastAPI app using AWS Elastic Beanstalk:\n\n1. **Install the AWS CLI**:\n   ```bash\n   pip install awsebcli\n   ```\n2. **Initialize Your Application**:\n   Inside your project directory:\n   ```bash\n   eb init -p python-3.8 fastapi-app --region <your-region>\n   ```\n3. **Create and Deploy the Application**:\n   ```bash\n   eb create fastapi-env\n   eb deploy\n   ```\n\nAWS gives you deep control over the configuration, security, and scaling of your application, which is perfect for enterprise-scale apps.\n\n#### Google Cloud Platform (GCP)\n\nGCP offers **App Engine**, **Compute Engine**, and **Cloud Run** for deploying FastAPI applications. **App Engine** is the easiest way to deploy apps without managing servers, while **Cloud Run** allows you to deploy containerized applications.\n\nDeploying your FastAPI app using **Google App Engine**:\n\n1. **Install the Google Cloud SDK**:\n   ```bash\n   curl https://sdk.cloud.google.com | bash\n   ```\n2. **Create the App Engine Configuration File (`app.yaml`)**:\n   ```yaml\n   runtime: python38\n   entrypoint: uvicorn main:app --host 0.0.0.0 --port $PORT\n   ```\n3. **Deploy the Application**:\n   ```bash\n   gcloud app deploy\n   ```\n\nGCP is known for its powerful machine learning services and data analytics tools, making it a great choice for apps that require heavy data processing.\n\n#### Azure\n\n**Azure App Service** and **Azure Kubernetes Service (AKS)** are the primary deployment platforms in the Microsoft cloud ecosystem. **Azure App Service** simplifies the deployment process while **AKS** offers enterprise-grade scalability for containerized applications.\n\nSteps to deploy using **Azure App Service**:\n\n1. **Install the Azure CLI**:\n   ```bash\n   az login\n   ```\n2. **Create an App Service Plan and Web App**:\n   ```bash\n   az webapp up --runtime \"PYTHON:3.8\" --name <app-name>\n   ```\n3. **Deploy the Application**:\n   Simply push your changes to the web app using Git or the Azure CLI.\n\n**Azure** offers deep integration with Microsoft's other tools and services, which is useful if you’re working in an enterprise environment already using Microsoft products.\n\n### 5.2 Using Container Orchestration Tools like Kubernetes\n\nDeploying individual Docker containers is great, but what if you want to scale up? That’s where **Kubernetes** comes into play. **Kubernetes** is the king of container orchestration. It helps you manage, scale, and maintain containerized applications across multiple servers (nodes).\n\nImagine Kubernetes as the traffic manager of your container city. It ensures that all traffic goes to the right containers (pods), scales the number of containers up or down based on demand, and keeps everything running smoothly.\n\n#### Why Use Kubernetes?\n\n- **Scalability**: Kubernetes automatically scales your application based on the number of requests.\n- **Self-Healing**: If one of your containers crashes, Kubernetes automatically restarts it.\n- **Load Balancing**: Kubernetes balances traffic across your containers so no one pod is overwhelmed.\n- **Deployment Rollbacks**: You can easily roll back to a previous version if something goes wrong.\n\n#### Deploying FastAPI on Kubernetes\n\nHere’s a basic overview of how to get your FastAPI app running on **Kubernetes**:\n\n1. **Create a Docker Image** for your FastAPI app (we’ve done this earlier).\n2. **Create a Kubernetes Deployment**:\n   ```yaml\n   apiVersion: apps/v1\n   kind: Deployment\n   metadata:\n     name: fastapi-app\n   spec:\n     replicas: 3\n     selector:\n       matchLabels:\n         app: fastapi\n     template:\n       metadata:\n         labels:\n           app: fastapi\n       spec:\n         containers:\n         - name: fastapi-container\n           image: <your-docker-image>\n           ports:\n           - containerPort: 80\n   ```\n\n3. **Create a Service** to expose your app to the internet:\n   ```yaml\n   apiVersion: v1\n   kind: Service\n   metadata:\n     name: fastapi-service\n   spec:\n     selector:\n       app: fastapi\n     ports:\n     - protocol: TCP\n       port: 80\n       targetPort: 80\n     type: LoadBalancer\n   ```\n\n4. **Deploy on Kubernetes**:\n   ```bash\n   kubectl apply -f deployment.yaml\n   kubectl apply -f service.yaml\n   ```\n\nYou can deploy Kubernetes clusters on **AWS (EKS)**, **GCP (GKE)**, or **Azure (AKS)**, giving you the power to scale across the cloud.\n\n### 5.3 Integrating with Managed Machine Learning Services\n\nAs data professionals, one of the coolest things you can do is integrate your FastAPI app with **managed machine learning services**. These cloud services take care of the heavy lifting, allowing you to scale and deploy machine learning models seamlessly.\n\n#### Why Use Managed ML Services?\n\n- **Simplified Infrastructure**: You don’t have to worry about setting up complex machine learning environments.\n- **Auto-Scaling**: Cloud providers automatically scale your ML models based on usage.\n- **Integrations**: These platforms come with tools for deploying, monitoring, and managing models in production.\n\nLet’s look at how you can integrate with the big three: AWS, GCP, and Azure.\n\n#### AWS SageMaker\n\n**SageMaker** is AWS’s fully managed machine learning service. You can build, train, and deploy ML models directly from SageMaker, and integrate the deployed model into your FastAPI app.\n\nSteps to integrate SageMaker with FastAPI:\n1. **Train and Deploy Your Model on SageMaker**.\n   ```python\n   import sagemaker\n   from sagemaker import get_execution_role\n\n   # Define the session and role\n   session = sagemaker.Session()\n   role = get_execution_role()\n\n   # Train a model\n   estimator = sagemaker.estimator.Estimator(...)\n   estimator.fit(...)\n\n   # Deploy the model\n   predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n   ```\n\n2. **Invoke the Model from FastAPI**:\n   You can invoke the deployed model in your FastAPI app using the SageMaker **Runtime API**:\n   ```python\n   import boto3\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       sagemaker_client = boto3.client('sagemaker-runtime')\n       response = sagemaker_client.invoke_endpoint(\n           EndpointName='<your-endpoint>',\n           ContentType='application/json',\n           Body=json.dumps(data.dict())\n       )\n       return response\n   ```\n\n#### Google AI Platform\n\nGCP’s **AI Platform** offers tools for training and serving machine learning models. You can train a model in **AI Platform** and deploy it to a managed endpoint.\n\n1. **Deploy Your Model** to AI Platform:\n   ```bash\n   gcloud ai-platform models create model_name\n   gcloud ai-platform versions create version_name \\\n       --model model_name \\\n       --origin gs://path-to-your-model\n   ```\n\n2. **Integrate with FastAPI**:\n   Use the GCP API to make predictions from your FastAPI app.\n   ```python\n   from google.cloud import aiplatform\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       client = aiplatform.gapic.PredictionServiceClient()\n       response = client.predict(endpoint='<your-endpoint>', ...)\n       return response\n   ```\n\n#### Azure Machine Learning (AML)\n\n**Azure Machine Learning (AML)** is another managed service that allows you to train and deploy ML models at scale.\n\n1. **Deploy Your Model** to Azure:\n   ```bash\n   az ml model deploy -n model_name -m model.pkl --ic inference_config.json --dc deployment_config.json\n   ```\n\n2. **Call the Model from FastAPI**:\n   ```python\n   import requests\n\n   @app.post(\"/predict\")\n   def predict(data: YourDataSchema):\n       response = requests.post('<your-aml-endpoint>', json=data.dict())\n       return response.json()\n   ```\n   \n## In Summary \n\nCongratulations on completing the course!🎉\n\nYou’ve just navigated through an exciting journey of deploying machine learning applications. \n\nHere’s a recap of what you’ve mastered:\n\n- **Containerization with Docker**: You learned how to package your applications into Docker containers, ensuring consistent and portable deployments across different environments.\n- **CI/CD Pipelines with GitHub Actions**: You explored the principles of Continuous Integration and Continuous Delivery (CI/CD), leveraging GitHub Actions to automate your deployment workflows and streamline your development process.\n- **Heroku Deployment**: You successfully deployed your applications to Heroku, making it simple to launch your projects into the cloud and manage them effortlessly.\n- **Best Practices for Deployment**: You discovered essential best practices to optimize, scale, and secure your deployments, ensuring that your applications perform well under pressure.\n- **Advanced Tools and Technologies**: You explored various cloud platforms like AWS, GCP, and Azure, delved into container orchestration with Kubernetes, and integrated with managed machine learning services to enhance your applications.\n\nWith this knowledge, you are now equipped to deploy robust, scalable machine learning applications and take on any challenge in the tech universe. The journey doesn’t end here; keep experimenting, learning, and pushing the boundaries of what’s possible! 🚀\n\nIt’s time to apply everything you’ve learned, take your applications to the next level, and impress the world with your data science skills. \n\nNow, let’s dive into the **[Lab 3](../01_Exercises/03_Lab.qmd)**"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"03_Lecture.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.554","code-summary":"Show the code","theme":{"light":["flatly","../styles.scss"]},"title":"Lecture 3 - Deployment and Production Considerations","subtitle":"Deploying Machine Learning Applications","author":"Ményssa Cherifa-Luron","date":"today"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}